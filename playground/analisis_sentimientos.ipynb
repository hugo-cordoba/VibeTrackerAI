{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/emoji_sentiment_dataset.csv')\n",
    "\n",
    "print(df)\n",
    "print(f\"El dataset tiene {df.shape[0]} filas\")\n",
    "print(f\"El dataset tiene {df.isnull().any(axis=1).sum()} filas con valores nulos\")\n",
    "df_clean = df.dropna()\n",
    "print(f\"El dataset tiene {df_clean.isnull().any(axis=1).sum()} filas con valores nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.groupby('category').count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_word_cloud(terms, category):\n",
    "    # Definir un t칤tulo basado en el valor de la categor칤a\n",
    "    if category == 1.0:\n",
    "        title = \"Positivo\"\n",
    "    elif category == 0.0:\n",
    "        title = \"Neutral\"\n",
    "    else:\n",
    "        title = \"Negativo\"\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(terms)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Nube de Palabras - Sentimiento {title}', fontsize=20)  # A침adir t칤tulo aqu칤\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Aseg칰rate de que los datos est치n limpios y los textos son strings\n",
    "df_clean['clean_text'] = df_clean['clean_text'].astype(str)\n",
    "\n",
    "# Generar nubes de palabras para cada categor칤a\n",
    "for category in [-1.0, 0.0, 1.0]:\n",
    "    subset = df_clean[df_clean['category'] == category]\n",
    "    texts = \" \".join(subset['clean_text'])\n",
    "    tfidf_vector = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    X_sub = tfidf_vector.fit_transform([texts])\n",
    "    max_words = {word: X_sub[0, idx] for word, idx in tfidf_vector.vocabulary_.items()}\n",
    "    plot_word_cloud(max_words, category)  # Pasar tambi칠n la categor칤a como argumento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaci칩n\n",
    "tokenizer = Tokenizer(num_words=5000)  # Solo considera las 5000 palabras m치s frecuentes\n",
    "tokenizer.fit_on_texts(df_clean['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df_clean['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "max_sequence_len = max([len(x) for x in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_len)\n",
    "y = to_categorical(np.asarray(df_clean['category'] + 1))  # Convertir -1, 0, 1 a 0, 1, 2 para categor칤as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_sequence_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    Dense(3, activation='softmax')  # 3 categor칤as de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, validation_split=0.1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(model.evaluate(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci칩n para obtener el nombre Unicode de un emoji\n",
    "def emoji_to_unicode_name(em):\n",
    "    return emoji.demojize(em)\n",
    "\n",
    "# Lista de emojis\n",
    "new_text = '游땘游땘游뱆游뱆仇벒잺仇벒잺游녟游땙游땙'\n",
    "\n",
    "# Convertir la lista a un pandas Series\n",
    "new_text_series = pd.Series(new_text)\n",
    "\n",
    "# Aplicar la funci칩n a la Series\n",
    "new_text_unicode_names = new_text_series.apply(emoji_to_unicode_name).str.replace(':',' ')\n",
    "\n",
    "\n",
    "# Convertir de nuevo a lista si es necesario\n",
    "new_text = new_text_unicode_names.tolist()\n",
    "\n",
    "print(new_text)\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(new_text)\n",
    "padded = pad_sequences(seq, maxlen=max_sequence_len)\n",
    "\n",
    "# Realizar la predicci칩n\n",
    "pred = model.predict(padded)\n",
    "print(f'Predicci칩n: {pred[0]}')  # Muestra la probabilidad de cada categor칤a\n",
    "\n",
    "# Determinar la categor칤a m치s probable\n",
    "predicted_category_index = np.argmax(pred[0])\n",
    "categories = {-1: 'Negativo', 0: 'Neutral', 1: 'Positivo'}  # Categor칤as ajustadas para corresponder a tus etiquetas\n",
    "predicted_category = categories[predicted_category_index - 1]  # ajustar el 칤ndice para -1, 0, 1\n",
    "print(f'La categor칤a m치s probable es: {predicted_category} con una probabilidad de {pred[0][predicted_category_index]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefreeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
